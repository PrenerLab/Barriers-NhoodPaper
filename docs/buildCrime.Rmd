---
title: "Build Crime Data Set"
author: "Christopher Prener, Ph.D."
date: '(`r format(Sys.time(), "%B %d, %Y")`)'
output: 
  github_document: default
  html_notebook: default 
---

## Introduction
This notebook creates the crime data set for further analysis.

## Dependencies
This notebook depends on the following packages:

```{r load-packages}
# primary data tools
library(compstatr)     # work with stlmpd crime data

# tidyverse packages
library(dplyr)         # data wrangling

# spatial packages
library(gateway)       # work with st. louis spatial data
library(ggmap)         # batch geocoding
library(sf)

# other packages
library(janitor)       # frequency tables
library(here)          # file path management
library(testthat)      # unit testing
```

## Create Data
Data downloaded from the STLMPD website come in `.csv` format but with the wrong file extension. The following bash script copies them to a new subdirectory and fixes the file extension issue:

```{bash}
# change working directory
cd ..

# execute cleaning script
bash source/reformatHTML.sh
```

## Load Data
With our data renamed, we build a year list objects for 2016, 2017, and 2017 crimes:

```{r load-data}
data2016 <- cs_load_year(here("data", "raw", "stlmpd", "csv", "2016"))
data2017 <- cs_load_year(here("data", "raw", "stlmpd", "csv", "2017"))
data2018 <- cs_load_year(here("data", "raw", "stlmpd", "csv", "2018"))
```

## Validate Data
Next we make sure there are no problems with the crime files in terms of incongruent columns:

```{r validate-data16}
cs_validate_year(data2016, year = "2016")
```

All of the data passes the validation checks.

```{r validate-data17}
cs_validate_year(data2017, year = "2017")
```

We can use the `verbose = TRUE` option on `cs_validate_year()` to identify areas where the validation checks have failed:

```{r validate-data17}
cs_validate_year(data2017, year = "2017", verbose = TRUE)
```

The data for May 2017 do not pass the validation checks. We can extract this month and confirm that there are too many columns in the May 2017 release. Once we have that confirmed, we can standardize that month and re-run our validation.

```{r fix-may-cols}
# extract data
may2017 <- cs_extract_month(data2017, month = "May")

# unit test column number
expect_equal(ncol(may2017), 26)

# remove object
rm(may2017)

# standardize months
data2017 <- cs_standardize(data2017, month = "May", config = 26)

# validate data
cs_validate_year(data2017, year = "2017")
```

We now get a `TRUE` value for `cs_validate_year()` and can move on to 2018 data.

```{r validate-data18}
cs_validate_year(data2018, year = "2018")
```



## Collapse Data
With the data validated, we collapse it into a single, flat object:

```{r collapse-data}
data2016_flat <- cs_collapse(data2016)
```

## Remove Unfounded Crimes and Subset Based on Type of Crime:
The following code chunk removes unfounded crimes (those where `Count == -1`) and then creates two data frames, one for violent crimes and one for all part one crimes:

```{r subset-data}
# violent crimes
data2016_flat %>% 
  cs_filter_count(var = Count) %>%
  cs_filter_crime(var = Crime, crime = "Violent") -> violentCrimes

# part 1 crimes
data2016_flat %>% 
  cs_filter_count(var = Count) %>%
  cs_filter_crime(var = Crime, crime = "Part 1") -> part1Crimes
```

## Check for and Address Missing Spatial Data
Before proceeding, we'll check for missing spatial data.

```{r check-xy-violent}
violentCrimes <- cs_missing_xy(violentCrimes, varx = XCoord, vary = YCoord, newVar = xyCheck)

table(violentCrimes$xyCheck)
```

About 6% of the violent crimes are missing spatial data.

```{r check-xy-p1}
part1Crimes <- cs_missing_xy(part1Crimes, varx = XCoord, vary = YCoord, newVar = xyCheck)

table(part1Crimes$xyCheck)
```

About 2% of the part 1 crimes are missing spatial data. Since these have the same root data, we'll pull out those observations that are missing spatial data and attempt to geocode them with the Google Maps API. 

We start with 494 observations

```{r geocode}
part1Crimes %>% 
  filter(xyCheck == TRUE) %>%
  mutate(fullAddress = paste0(ILEADSAddress, " ", ILEADSStreet, ", St. Louis, MO" )) -> part1Crimes_miss

part1Crimes %>% 
  filter(xyCheck == FALSE) -> part1Crimes_valid

nrow(part1Crimes_miss)
```

Removing the truly "unknown" addresses yields a set of 429 addresses that could potentially be geocoded.

```{r tabulate-missinXY}
part1Crimes_miss %>%
  tabyl(Description)
```

Rape incidents comprise almost 66% of these incidents that cannot be located.

```{r geocode-missingXY, eval=FALSE}
source(here("source", "geocodeCrimes.R"))
```

## Project Both Sets of Data

- need to filter out prior crimes
- need to pull in data from 2017 / 2018 and search for 2016 crimes

```{r project-valid}
x <- st_as_sf(part1Crimes_valid, coords = c("XCoord", "YCoord"), crs = 102696)
```


```{r project-geocoded}

```


```{r combine-sf-objects}

```

## Export Data


```{r export-data}

```

